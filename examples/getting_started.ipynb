{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add parent directory to path to import src modules\n",
    "import sys\n",
    "sys.path.append('../')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from ray.rllib.core.rl_module.rl_module import RLModule\n",
    "from ray.rllib.core.rl_module.multi_rl_module import MultiRLModule\n",
    "import torch\n",
    "from ray.rllib.core.columns import Columns\n",
    "\n",
    "from src.agent.battery import Battery\n",
    "from src.agent.der import DERAgent\n",
    "from src.environment.train import RLAlgorithm, RLTrainer, TrainingMode\n",
    "from src.grid.base import GridTopology\n",
    "from src.grid.network import GridNetwork\n",
    "from src.market.matching import ClearingMechanism, MarketConfig\n",
    "from src.profile.der import DERProfileHandler\n",
    "from src.profile.dso import DSOProfileHandler\n",
    "from src.market.dso import DSOAgent\n",
    "from src.environment.inference import RLInference\n",
    "from src.environment.io import EnvConfigHandler\n",
    "from src.root import __main__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üöÄ Beginner's Guide: Local Energy Market Environment\n",
    "\n",
    "This notebook provides a step-by-step guide to setting up and using the Local Energy Market (LEM) environment for reinforcement learning. You'll learn how to:\n",
    "\n",
    "1. **Configure the environment** - Set up market parameters, agents, and grid network\n",
    "2. **Train RL agents** - Train agents using different algorithms and training modes\n",
    "3. **Run inference** - Evaluate trained agents in the environment\n",
    "\n",
    "Each section builds on the previous one, so follow along sequentially.\n",
    "\n",
    "---\n",
    "\n",
    "## üìã Table of Contents\n",
    "\n",
    "1. [Initial Setup](#1-initial-setup)\n",
    "   - [Market Configuration](#11-market-configuration-marketconfig)\n",
    "   - [DER Profile Handler](#12-der-profile-handler-derprofilehandler)\n",
    "   - [DSO Profile Handler](#13-dso-profile-handler-dsoprofilehandler)\n",
    "   - [DER + Battery](#14-der--battery-deragent--battery)\n",
    "   - [Grid Network](#15-grid-network-gridnetwork)\n",
    "   - [DSO Agent](#16-dso-dsoagent)\n",
    "   - [Environment Configuration](#17-environment-configuration)\n",
    "2. [RL Training](#2-rl-training-rltrainer)\n",
    "3. [Inference](#3-inference-rlinference)\n",
    "\n",
    "---\n",
    "\n",
    "## 1. üõ†Ô∏è Initial Setup\n",
    "\n",
    "In this section, we'll configure all the necessary components for the Local Energy Market environment. This includes setting up the market rules, creating agents with energy profiles, configuring the grid network, and establishing the DSO (Distribution System Operator)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# Configuration Parameters\n",
    "# ============================================================================\n",
    "\n",
    "# Environment Configuration\n",
    "STEPS = 24              # Number of timesteps per episode (e.g., 24 = 24 hours)\n",
    "NUM_AGENTS = 3          # Number of DER agents participating in the market\n",
    "SEED = 42               # Random seed for reproducibility\n",
    "\n",
    "# Training Configuration\n",
    "ITERS_TRAIN = 3         # Number of training iterations (for quick testing; use more in production)\n",
    "TUNE_SAMPLES = 1        # Number of hyperparameter tuning samples\n",
    "ALGO = \"sac\"            # RL algorithm: \"ppo\", \"appo\", or \"sac\"\n",
    "MODE = \"ctce\"           # Training mode: \"ctce\" (centralized training, centralized execution),\n",
    "                        #                \"ctde\" (centralized training, decentralized execution),\n",
    "                        #                \"dtde\" (decentralized training, decentralized execution)\n",
    "STORAGE_PATH = \"/Users/nasalazar/Documentos/GitHub/simulations/phd/downloads\"\n",
    "\n",
    "# Checkpoint Restoration (for continuing training)\n",
    "EXPERIMENT_PATH = \"/Users/nasalazar/Documentos/GitHub/simulations/phd/downloads/TRAIN/lem_ctce_sac_06September1341\"\n",
    "CHECKPOINT_PATH_TRAIN = \"/Users/nasalazar/Documentos/GitHub/simulations/phd/downloads/TRAIN/lem_ctce_sac_06September1341/SAC_GroupedLEM_166ac_00000_0_2025-09-06_13-41-23/checkpoint_000002\"\n",
    "EMBEDDINGS_DIM = 128    # Dimension of agent embeddings (for CTDE/DTDE modes)\n",
    "\n",
    "# Inference Configuration\n",
    "ITERS_INFERENCE = 3     # Number of inference episodes to run\n",
    "EXPLORATION = False     # Whether to use exploration during inference (False = use best policy)\n",
    "CHECKPOINT_PATH_INFERENCE = \"/Users/nasalazar/Documentos/GitHub/simulations/phd/downloads/INFERENCE/lem_ctce_sac_06September1341/SAC_GroupedLEM_166ac_00000_0_2025-09-06_13-41-23/checkpoint_000002\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1. üí∞ Market Configuration (`MarketConfig`)\n",
    "\n",
    "The `MarketConfig` defines the rules and constraints of the local energy market. This is the foundation of your market setup.\n",
    "\n",
    "**Key Components:**\n",
    "- **Price bounds** (min/max prices) - Define the price range for energy trading\n",
    "- **Quantity bounds** (min/max trade quantities) - Set limits on trade sizes\n",
    "- **Price clearing mechanism** - Determines how market price is calculated\n",
    "- **Blockchain settings** - Configures decentralized validation (optional)\n",
    "- **Partner preference** - Enables strategic trading relationships\n",
    "\n",
    "**Available Clearing Mechanisms:**\n",
    "- `AVERAGE`: Average of matched bid/ask prices\n",
    "- `FIRST_PRICE`: Price of first matched order\n",
    "- `LAST_PRICE`: Price of last matched order\n",
    "- `HIGHEST_BID`: Highest bid price\n",
    "- `LOWEST_ASK`: Lowest ask price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "market_config = MarketConfig(\n",
    "    min_price=0.0,                          # Minimum allowed price per unit of energy\n",
    "    max_price=100.0,                        # Maximum allowed price per unit of energy\n",
    "    min_quantity=0.0,                      # Minimum allowed trade quantity\n",
    "    max_quantity=100.0,                     # Maximum allowed trade quantity\n",
    "    price_mechanism=ClearingMechanism.AVERAGE,  # How market clearing price is determined:\n",
    "                                                 #   AVERAGE: Average of matched bid/ask prices\n",
    "                                                 #   FIRST_PRICE: Price of first matched order\n",
    "                                                 #   LAST_PRICE: Price of last matched order\n",
    "                                                 #   HIGHEST_BID: Highest bid price\n",
    "                                                 #   LOWEST_ASK: Lowest ask price\n",
    "    blockchain_difficulty=2,                # Proof-of-work difficulty for blockchain validation\n",
    "    visualize_blockchain=False,            # Whether to visualize blockchain structure\n",
    "    enable_partner_preference=True          # Allow agents to prefer trading with specific partners\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2. üìä DER Profile Handler (`DERProfileHandler`)\n",
    "\n",
    "The `DERProfileHandler` generates energy profiles (generation and demand) for DER agents.\n",
    "\n",
    "**Capabilities:**\n",
    "- ‚úÖ Generate random profiles based on capacity constraints\n",
    "- ‚úÖ Load profiles from CSV files (if file paths are provided)\n",
    "- ‚úÖ Ensure profiles respect market quantity bounds\n",
    "\n",
    "**Profile Types:**\n",
    "- **Generation Profile**: Energy the agent produces (e.g., solar panels, wind turbines)\n",
    "- **Demand Profile**: Energy the agent consumes (e.g., household/business load)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "der_profile_handler = DERProfileHandler(\n",
    "    min_quantity=market_config.min_quantity,    # Minimum quantity bound (from market config)\n",
    "    max_quantity=market_config.max_quantity,    # Maximum quantity bound (from market config)\n",
    "    generation_file_path=None,                  # Path to CSV file with generation profiles\n",
    "                                                #   (None = generate random profiles)\n",
    "    demand_file_path=None,                      # Path to CSV file with demand profiles\n",
    "                                                #   (None = generate random profiles)\n",
    "    seed=SEED                                   # Random seed for reproducible profile generation\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3. üè¢ DSO Profile Handler (`DSOProfileHandler`)\n",
    "\n",
    "The `DSOProfileHandler` generates price profiles for the Distribution System Operator (DSO).\n",
    "\n",
    "**DSO Price Types:**\n",
    "- **Feed-in tariff** üí∞: Price the DSO pays agents for excess energy they generate\n",
    "- **Utility price** üíµ: Price agents pay the DSO when buying energy from the grid\n",
    "\n",
    "**Profile Sources:**\n",
    "- Can be loaded from CSV files (real-world data)\n",
    "- Can be generated randomly (for simulation/testing)\n",
    "\n",
    "The DSO acts as a fallback market when local peer-to-peer trading cannot satisfy all energy needs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dso_profile_handler = DSOProfileHandler(\n",
    "    min_price=market_config.min_price,         # Minimum price bound (from market config)\n",
    "    max_price=market_config.max_price,         # Maximum price bound (from market config)\n",
    "    feed_in_tariff_file_path=None,             # Path to CSV file with feed-in tariff prices\n",
    "                                               #   (None = generate random prices)\n",
    "    utility_price_file_path=None,              # Path to CSV file with utility prices\n",
    "                                               #   (None = generate random prices)\n",
    "    seed=SEED                                  # Random seed for reproducible price generation\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4. üë• DER + Battery (`DERAgent` + `Battery`)\n",
    "\n",
    "Each DER agent represents a participant in the local energy market.\n",
    "\n",
    "**Agent Components:**\n",
    "- **Generation profile** ‚òÄÔ∏è: Energy the agent produces (e.g., from solar panels, wind turbines)\n",
    "- **Demand profile** üè†: Energy the agent consumes (e.g., household/business load)\n",
    "- **Battery** üîã: Optional energy storage system for time-shifting energy\n",
    "\n",
    "**Battery Capabilities:**\n",
    "- **Charge**: Store excess energy when generation exceeds demand\n",
    "- **Discharge**: Release stored energy when demand exceeds generation\n",
    "- **Time-shifting**: Enable coordination across different time periods\n",
    "\n",
    "We create multiple agents with randomized capacities and profiles to simulate a diverse, realistic market environment. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agents = []\n",
    "\n",
    "for i in range(NUM_AGENTS):\n",
    "    # Generate random capacities for diversity in the market\n",
    "    der_capacity = np.random.randint(100, 200)      # DER capacity (e.g., solar panel kW)\n",
    "    battery_capacity = der_capacity // 2            # Battery capacity (half of DER capacity)\n",
    "\n",
    "    # Generate energy profiles (generation and demand) for this agent\n",
    "    generation, demand = der_profile_handler.get_energy_profiles(\n",
    "        steps=STEPS,                                 # Number of timesteps\n",
    "        capacity=der_capacity + battery_capacity,    # Total capacity for profile scaling\n",
    "        constant=bool(np.random.choice([True, False]))  # Whether profile is constant or variable\n",
    "    )\n",
    "\n",
    "    # Create battery storage system\n",
    "    battery = Battery(\n",
    "        nominal_capacity=battery_capacity,           # Maximum energy storage capacity\n",
    "        min_soc=0.0,                                # Minimum state of charge (0 = empty)\n",
    "        max_soc=1.0,                                # Maximum state of charge (1 = full)\n",
    "        charge_efficiency=0.95,                      # Efficiency when charging (95% = 5% loss)\n",
    "        discharge_efficiency=0.95                   # Efficiency when discharging (95% = 5% loss)\n",
    "    )\n",
    "\n",
    "    # Create DER agent with generation, demand, and battery\n",
    "    agent = DERAgent(\n",
    "        id=f\"agent_{i}\",                            # Unique agent identifier\n",
    "        capacity=der_capacity,                       # Maximum generation capacity\n",
    "        battery=battery,                             # Battery storage system\n",
    "        node_id=None,                                # Grid node ID (None = auto-assign)\n",
    "        generation_profile=generation,               # Time series of energy generation\n",
    "        demand_profile=demand                        # Time series of energy demand\n",
    "    )\n",
    "\n",
    "    agents.append(agent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.5. üîå Grid Network (`GridNetwork`)\n",
    "\n",
    "The `GridNetwork` represents the physical electrical grid infrastructure.\n",
    "\n",
    "**Grid Functions:**\n",
    "- **Topology definition** üó∫Ô∏è: Network structure of the grid (how agents are connected)\n",
    "- **Distance calculations** üìè: Calculates distances between agents for transmission loss\n",
    "- **Constraint validation** ‚ö†Ô∏è: Validates grid constraints (capacity, voltage, etc.)\n",
    "\n",
    "**Available Topologies:**\n",
    "- **IEEE34**: Standard IEEE test feeder (34 nodes)\n",
    "- **MESH**: Fully connected network\n",
    "- **RADIAL**: Tree-like structure\n",
    "- **RING**: Circular connection pattern\n",
    "- Custom topologies for specific scenarios\n",
    "\n",
    "The grid topology affects transmission losses and trading opportunities between agents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate total capacity of all agents for grid sizing\n",
    "capacity = sum([a.capacity for a in agents])\n",
    "\n",
    "grid_network = GridNetwork(\n",
    "    topology=GridTopology.IEEE34,    # Grid topology type (IEEE34 is a standard test feeder)\n",
    "    num_nodes=34,                     # Number of nodes in the grid network\n",
    "    capacity=capacity,                # Total grid capacity (sum of all agent capacities)\n",
    "    seed=SEED                         # Random seed for grid node assignment\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.6. üè¢ DSO (`DSOAgent`)\n",
    "\n",
    "The Distribution System Operator (DSO) agent manages the grid and provides fallback trading.\n",
    "\n",
    "**DSO Responsibilities:**\n",
    "- **Feed-in tariff** üí∞: Buys excess energy from agents at a fixed rate\n",
    "- **Utility price** üíµ: Sells energy to agents when local market cannot satisfy demand\n",
    "- **Grid management** ‚ö°: Maintains grid balance and handles unmatched orders\n",
    "\n",
    "**DSO Role:**\n",
    "The DSO acts as a safety net for the local energy market, ensuring that:\n",
    "- Agents can always sell excess energy (even if no local buyers exist)\n",
    "- Agents can always buy energy (even if no local sellers exist)\n",
    "- Grid stability is maintained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get price profiles for the DSO (feed-in tariff and utility price)\n",
    "fit, utility = dso_profile_handler.get_price_profiles(steps=STEPS)\n",
    "\n",
    "dso = DSOAgent(\n",
    "    id=\"dso\",                           # DSO identifier\n",
    "    feed_in_tariff=fit,                 # Time series of prices DSO pays for excess energy\n",
    "    utility_price=utility,               # Time series of prices agents pay for grid energy\n",
    "    grid_network=grid_network,           # Grid network for distance/constraint calculations\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.7. ‚öôÔ∏è Environment Configuration\n",
    "\n",
    "Now we combine all components into a complete environment configuration dictionary that will be used by the RL training and inference systems.\n",
    "\n",
    "**Configuration Components:**\n",
    "- ‚úÖ Market rules and constraints\n",
    "- ‚úÖ Agent setup with profiles and batteries\n",
    "- ‚úÖ Grid network topology\n",
    "- ‚úÖ DSO price profiles\n",
    "- ‚úÖ Environment behavior settings\n",
    "\n",
    "This configuration dictionary will be passed to the RL trainer and inference systems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_config = {\n",
    "    \"max_steps\": STEPS,                         # Maximum number of timesteps per episode\n",
    "    \"agents\": agents,                           # List of DER agents\n",
    "    \"market_config\": market_config,             # Market configuration\n",
    "    \"grid_network\": grid_network,               # Grid network topology\n",
    "    \"dso\": dso,                                  # Distribution System Operator agent\n",
    "    \"der_profile_handler\": der_profile_handler, # Handler for generating DER energy profiles\n",
    "    \"dso_profile_handler\": dso_profile_handler, # Handler for generating DSO price profiles\n",
    "    \"enable_reset_dso_profiles\": False,         # Whether to reset DSO profiles on env reset\n",
    "    \"enable_asynchronous_order\": True,          # Whether agents submit orders asynchronously\n",
    "    \"max_error\": 0.3,                           # Maximum allowed error in market clearing\n",
    "    \"num_anchor\": 4,                            # Number of anchor points for reputation system\n",
    "    \"seed\": SEED                                # Random seed for environment reproducibility\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: Save environment configuration to file for later use\n",
    "EnvConfigHandler.save(env_config,\n",
    "                      STORAGE_PATH,\n",
    "                      \"env_config2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. üéì RL Training (`RLTrainer`)\n",
    "\n",
    "In this section, we'll set up and run reinforcement learning training. The `RLTrainer` supports multiple algorithms and training modes.\n",
    "\n",
    "## üìö Training Modes Explained\n",
    "\n",
    "**CTCE** (Centralized Training, Centralized Execution):\n",
    "- Single shared policy across all agents\n",
    "- Centralized execution\n",
    "- Best for: Homogeneous agents, simpler coordination\n",
    "\n",
    "**CTDE** (Centralized Training, Decentralized Execution):\n",
    "- Centralized training (shared experience)\n",
    "- Each agent has its own policy\n",
    "- Best for: Heterogeneous agents, independent decision-making\n",
    "\n",
    "**DTDE** (Decentralized Training, Decentralized Execution):\n",
    "- Fully decentralized training\n",
    "- Each agent trains independently\n",
    "- Best for: Privacy-preserving scenarios, realistic deployment\n",
    "\n",
    "## üß† Available Algorithms\n",
    "\n",
    "- **PPO** (Proximal Policy Optimization): Stable, sample-efficient\n",
    "- **APPO** (Asynchronous PPO): Faster training with parallel workers\n",
    "- **SAC** (Soft Actor-Critic): Off-policy, good for continuous actions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1. Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create RL trainer with specified algorithm and training mode\n",
    "trainer = RLTrainer(\n",
    "    env_config=env_config,                    # Environment configuration from section 1\n",
    "    algorithm=(RLAlgorithm.PPO if ALGO == \"ppo\" \n",
    "               else RLAlgorithm.APPO if ALGO == \"appo\" \n",
    "               else RLAlgorithm.SAC if ALGO == \"sac\" \n",
    "               else None),                    # RL algorithm: PPO, APPO, or SAC\n",
    "    training=(TrainingMode.CTDE if MODE == \"ctde\" \n",
    "              else TrainingMode.CTCE if MODE == \"ctce\" \n",
    "              else TrainingMode.DTDE if MODE == \"dtde\" \n",
    "              else None),                     # Training mode: CTCE, CTDE, or DTDE\n",
    "    iters=ITERS_TRAIN,                        # Number of training iterations\n",
    "    tune_samples=TUNE_SAMPLES,                # Number of hyperparameter tuning samples\n",
    "    checkpoint_freq=2,                        # Save checkpoint every N iterations\n",
    "    evaluation_interval=1,                    # Run evaluation every N iterations\n",
    "    evaluation_duration=3,                    # Number of episodes per evaluation\n",
    "    cpus=1,                                   # Number of CPU cores to use\n",
    "    gpus=0,                                   # Number of GPUs to use (0 = CPU only)\n",
    "    storage_path=STORAGE_PATH                 # Path where checkpoints and results are saved\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the environment by running a few steps with random actions\n",
    "# This verifies that the environment is properly configured before training\n",
    "for step in range(3):\n",
    "    # Generate random valid actions for all agents\n",
    "    actions = {}\n",
    "    for agent_id in trainer.env.agents:\n",
    "        # Handle different action space formats for different training modes\n",
    "        if hasattr(trainer.env, 'action_spaces') and trainer.env.action_spaces is not None:\n",
    "            action_space = trainer.env.action_spaces[agent_id]  # DTDE mode\n",
    "        else:\n",
    "            action_space = trainer.env.action_space[agent_id]   # CTCE/CTDE mode\n",
    "        actions[agent_id] = action_space.sample()\n",
    "\n",
    "    # Step the environment and observe rewards\n",
    "    obs, rewards, terminated, truncated, info = trainer.env.step(actions)\n",
    "    print(f\"Step {step + 1} | Reward: {rewards}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2. üöÄ Training\n",
    "\n",
    "Start the training process. This will:\n",
    "1. ‚úÖ Initialize the RL algorithm with the specified configuration\n",
    "2. üéì Train agents for the specified number of iterations\n",
    "3. üíæ Save checkpoints periodically (for recovery and analysis)\n",
    "4. üìä Run evaluations to track performance\n",
    "\n",
    "**‚ö†Ô∏è Note**: Uncomment the line below to start training. Training may take a while depending on:\n",
    "- Number of training iterations\n",
    "- System resources (CPU/GPU)\n",
    "- Number of agents\n",
    "- Environment complexity\n",
    "\n",
    "**üí° Tip**: Start with a small number of iterations (e.g., 3-5) to test your setup before running longer training sessions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start training (uncomment to run)\n",
    "# results, metrics = trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3. üîÑ Restore Experiment\n",
    "\n",
    "If you want to continue working with a previously trained experiment, you can restore it here.\n",
    "\n",
    "**Use Cases:**\n",
    "- üìä Analyze results from a previous training run\n",
    "- üéì Continue training from where you left off\n",
    "- üîç Inspect model checkpoints\n",
    "- üìà Compare different training configurations\n",
    "\n",
    "**Requirements:**\n",
    "- Experiment path must point to a valid training directory\n",
    "- Environment configuration should match the original training setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Restore a previously trained experiment (uncomment to use)\n",
    "# trainer.restore_experiment(\n",
    "#     experiment_path=EXPERIMENT_PATH,      # Path to the experiment directory\n",
    "#     embeddings_dim=EMBEDDINGS_DIM,        # Dimension of agent embeddings (for CTDE/DTDE)\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4. ‚ûï Continue Training a Checkpoint\n",
    "\n",
    "Continue training from a specific checkpoint. This is useful when you want to:\n",
    "- üéØ Fine-tune a trained model\n",
    "- üîÑ Continue training that was interrupted\n",
    "- üìà Train for additional iterations\n",
    "- üî¨ Experiment with extended training\n",
    "\n",
    "**When to Use:**\n",
    "- Your training was interrupted and you want to resume\n",
    "- You want to fine-tune a model with more training\n",
    "- You're experimenting with different training durations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Continue training from a checkpoint (uncomment to use)\n",
    "# trainer.train_checkpoint(\n",
    "#     checkpoint_path=CHECKPOINT_PATH_TRAIN,  # Path to the checkpoint file\n",
    "#     iters=3,                                 # Additional training iterations\n",
    "#     embeddings_dim=128                       # Dimension of agent embeddings (for CTDE/DTDE)\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. üî¨ Inference (`RLInference`)\n",
    "\n",
    "After training, use `RLInference` to evaluate trained agents in the environment.\n",
    "\n",
    "**Inference Features:**\n",
    "- üéØ **Deterministic Evaluation**: Runs the trained policy without exploration (default)\n",
    "- üé≤ **Exploration Mode**: Can enable exploration for testing robustness\n",
    "- üìä **Performance Metrics**: Collects statistics on agent performance\n",
    "- üíæ **Results Saving**: Automatically saves inference results to storage path\n",
    "\n",
    "**What Happens During Inference:**\n",
    "1. Loads the trained model from checkpoint\n",
    "2. Runs episodes in the environment\n",
    "3. Collects rewards, actions, and market statistics\n",
    "4. Saves results for analysis\n",
    "\n",
    "**üí° Tip**: Set `exploration=False` to evaluate the best learned policy, or `exploration=True` to test agent robustness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create inference model from a trained checkpoint\n",
    "model = RLInference(\n",
    "    env_config=env_config,                    # Environment configuration (must match training)\n",
    "    exploration=EXPLORATION,                  # Whether to use exploration (False = use best policy)\n",
    "    checkpoint_path=CHECKPOINT_PATH_INFERENCE, # Path to the trained checkpoint\n",
    "    storage_path=STORAGE_PATH                 # Path where inference results will be saved\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run inference for the specified number of episodes\n",
    "# This will evaluate the trained agents and save results to the storage path\n",
    "model.inference(ITERS_INFERENCE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üìù Summary & Next Steps\n",
    "\n",
    "### ‚úÖ What You've Learned\n",
    "\n",
    "This notebook demonstrated the complete workflow for:\n",
    "\n",
    "1. **‚öôÔ∏è Configuration**: Setting up market, agents, grid, and DSO\n",
    "2. **üéì Training**: Training RL agents using different algorithms and modes\n",
    "3. **üî¨ Inference**: Evaluating trained agents in the environment\n",
    "\n",
    "### üöÄ Next Steps\n",
    "\n",
    "**Experimentation:**\n",
    "- üîß Experiment with different market configurations and agent setups\n",
    "- üß† Try different RL algorithms (PPO, APPO, SAC) and training modes (CTCE, CTDE, DTDE)\n",
    "- üìä Analyze the saved results from training and inference\n",
    "- üìÅ Check the `downloads/` directory for saved checkpoints and metrics\n",
    "\n",
    "**Advanced Learning:**\n",
    "- üìö For more advanced examples, see the `cases/` directory for case study notebooks\n",
    "- üìñ Explore case studies on market mechanisms, agent heterogeneity, DSO intervention, and more\n",
    "\n",
    "### üìä Key Takeaways\n",
    "\n",
    "- **Market Configuration** defines trading rules and constraints\n",
    "- **Agent Profiles** determine energy generation and demand patterns\n",
    "- **Batteries** enable time-shifting and improved coordination\n",
    "- **Grid Network** affects transmission losses and trading opportunities\n",
    "- **DSO** provides fallback trading when local market cannot satisfy needs\n",
    "- **Training Modes** (CTCE, CTDE, DTDE) offer different coordination paradigms\n",
    "- **RL Algorithms** (PPO, APPO, SAC) have different strengths and use cases\n",
    "\n",
    "---\n",
    "\n",
    "**üéØ Ready to explore more? Check out the case studies in the `cases/` directory!**\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "phd",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
