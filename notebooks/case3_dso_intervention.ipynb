{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Case Study 3: DSO Intervention Strategies\n",
        "\n",
        "This notebook examines how different Distribution System Operator (DSO) intervention strategies and policies affect agent behavior, market participation, and grid stability in decentralized local energy markets.\n",
        "\n",
        "## üìã Table of Contents\n",
        "\n",
        "1. [Research Questions & Hypothesis](#research-questions--hypothesis)\n",
        "2. [Setup & Imports](#setup--imports)\n",
        "3. [Scenario Configuration](#scenario-configuration)\n",
        "4. [DSO Intervention Strategies](#dso-intervention-strategies)\n",
        "5. [Agent Creation](#agent-creation)\n",
        "6. [Training & Evaluation](#training--evaluation)\n",
        "7. [Grid Stability Analysis](#grid-stability-analysis)\n",
        "8. [Results Analysis](#results-analysis)\n",
        "9. [Research Implications](#research-implications)\n",
        "\n",
        "---\n",
        "\n",
        "## üî¨ Research Questions & Hypothesis\n",
        "\n",
        "### Research Questions Addressed:\n",
        "- How do different DSO intervention thresholds affect market participation?\n",
        "- What is the impact of DSO pricing policies on agent strategies?\n",
        "- How do grid constraints enforced by DSO influence coordination?\n",
        "- What balance between market freedom and grid stability is optimal?\n",
        "- How do agents adapt to varying levels of DSO strictness?\n",
        "\n",
        "### Hypothesis:\n",
        "More strict DSO intervention will improve grid stability but may reduce market efficiency and agent participation, while lenient policies may lead to grid constraints but higher economic efficiency.\n",
        "\n",
        "### DSO Intervention Strategies Tested:\n",
        "1. **Permissive:** Minimal intervention, high market freedom\n",
        "2. **Moderate:** Balanced intervention with standard thresholds\n",
        "3. **Strict:** High intervention with strict grid constraints\n",
        "4. **Dynamic:** Adaptive intervention based on real-time conditions\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üõ†Ô∏è Setup & Imports\n",
        "\n",
        "Let's import all necessary libraries and set up the environment for our DSO intervention analysis.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Standard library imports\n",
        "import sys\n",
        "import os\n",
        "import warnings\n",
        "from dataclasses import dataclass\n",
        "from typing import Any, Dict, List\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from pathlib import Path\n",
        "\n",
        "# Add project root to path\n",
        "project_root = Path.cwd().parent\n",
        "sys.path.append(str(project_root))\n",
        "\n",
        "# Suppress warnings for cleaner output\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Set up plotting style\n",
        "plt.style.use('seaborn-v0_8')\n",
        "sns.set_palette(\"husl\")\n",
        "\n",
        "print(\"‚úÖ Imports successful!\")\n",
        "print(f\"üìÅ Project root: {project_root}\")\n",
        "print(f\"üêç Python version: {sys.version}\")\n",
        "print(f\"üìä NumPy version: {np.__version__}\")\n",
        "print(f\"üìà Pandas version: {pd.__version__}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import project-specific modules\n",
        "try:\n",
        "    from src.agent.battery import Battery\n",
        "    from src.agent.der import DERAgent\n",
        "    from src.grid.network import GridNetwork, GridTopology\n",
        "    from src.market.dso import DSOAgent\n",
        "    from src.market.matching import MarketConfig\n",
        "    from src.market.mechanism import ClearingMechanism\n",
        "    from src.profile.der import DERProfileHandler\n",
        "    from src.profile.dso import DSOProfileHandler\n",
        "    from src.environment.train import RLTrainer, TrainingMode, RLAlgorithm\n",
        "    \n",
        "    print(\"‚úÖ Project modules imported successfully!\")\n",
        "    \n",
        "    # Display available clearing mechanisms\n",
        "    print(\"\\nüìã Available Clearing Mechanisms:\")\n",
        "    for mechanism in ClearingMechanism:\n",
        "        print(f\"  - {mechanism.name}: {mechanism.value}\")\n",
        "        \n",
        "except ImportError as e:\n",
        "    print(f\"‚ùå Error importing project modules: {e}\")\n",
        "    print(\"Please ensure you're running this notebook from the correct directory\")\n",
        "    print(\"and that all dependencies are installed.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ‚öôÔ∏è Scenario Configuration\n",
        "\n",
        "Let's define the base configuration parameters for our DSO intervention analysis. These parameters will be kept constant across all intervention strategies to ensure fair comparison.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "@dataclass\n",
        "class Case3Scenarios:\n",
        "    \"\"\"Case 3: DSO Intervention Strategies scenarios configuration.\"\"\"\n",
        "    \n",
        "    # Base simulation parameters\n",
        "    NUM_AGENTS = 8\n",
        "    MAX_STEPS = 24  # 24-hour simulation\n",
        "    GRID_CAPACITY = 1500.0  # kW\n",
        "    \n",
        "    # Market parameters\n",
        "    MIN_PRICE = 40.0  # $/MWh\n",
        "    MAX_PRICE = 250.0  # $/MWh\n",
        "    MIN_QUANTITY = 0.1  # kWh\n",
        "    MAX_QUANTITY = 200.0  # kWh\n",
        "\n",
        "# Display configuration\n",
        "print(\"üìä Case 3 Configuration:\")\n",
        "print(f\"  Number of Agents: {Case3Scenarios.NUM_AGENTS}\")\n",
        "print(f\"  Simulation Length: {Case3Scenarios.MAX_STEPS} hours\")\n",
        "print(f\"  Grid Capacity: {Case3Scenarios.GRID_CAPACITY} kW\")\n",
        "print(f\"  Price Range: ${Case3Scenarios.MIN_PRICE} - ${Case3Scenarios.MAX_PRICE} /MWh\")\n",
        "print(f\"  Quantity Range: {Case3Scenarios.MIN_QUANTITY} - {Case3Scenarios.MAX_QUANTITY} kWh\")\n",
        "print(f\"  DSO Strategies: 4 (Permissive, Moderate, Strict, Dynamic)\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üèõÔ∏è DSO Intervention Strategies\n",
        "\n",
        "Now let's define the four different DSO intervention strategies to understand how regulatory approaches affect market dynamics and grid stability.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def create_permissive_dso() -> DSOAgent:\n",
        "    \"\"\"Create permissive DSO with minimal intervention.\"\"\"\n",
        "    return DSOAgent(\n",
        "        intervention_threshold=0.95,  # High threshold - intervene only at 95% capacity\n",
        "        penalty_multiplier=1.2,        # Low penalty multiplier\n",
        "        price_adjustment_factor=0.1,   # Minimal price adjustments\n",
        "        grid_stability_weight=0.3,     # Low weight on grid stability\n",
        "        market_efficiency_weight=0.7,  # High weight on market efficiency\n",
        "        intervention_frequency=0.1,    # Low intervention frequency\n",
        "        adaptive_threshold=False       # Fixed threshold\n",
        "    )\n",
        "\n",
        "def create_moderate_dso() -> DSOAgent:\n",
        "    \"\"\"Create moderate DSO with balanced intervention.\"\"\"\n",
        "    return DSOAgent(\n",
        "        intervention_threshold=0.8,    # Moderate threshold - intervene at 80% capacity\n",
        "        penalty_multiplier=1.5,        # Moderate penalty multiplier\n",
        "        price_adjustment_factor=0.2,   # Moderate price adjustments\n",
        "        grid_stability_weight=0.5,     # Balanced weights\n",
        "        market_efficiency_weight=0.5, # Balanced weights\n",
        "        intervention_frequency=0.3,    # Moderate intervention frequency\n",
        "        adaptive_threshold=False       # Fixed threshold\n",
        "    )\n",
        "\n",
        "def create_strict_dso() -> DSOAgent:\n",
        "    \"\"\"Create strict DSO with high intervention.\"\"\"\n",
        "    return DSOAgent(\n",
        "        intervention_threshold=0.7,    # Low threshold - intervene at 70% capacity\n",
        "        penalty_multiplier=2.0,        # High penalty multiplier\n",
        "        price_adjustment_factor=0.3,   # High price adjustments\n",
        "        grid_stability_weight=0.8,     # High weight on grid stability\n",
        "        market_efficiency_weight=0.2,  # Low weight on market efficiency\n",
        "        intervention_frequency=0.5,    # High intervention frequency\n",
        "        adaptive_threshold=False       # Fixed threshold\n",
        "    )\n",
        "\n",
        "def create_dynamic_dso() -> DSOAgent:\n",
        "    \"\"\"Create dynamic DSO with adaptive intervention.\"\"\"\n",
        "    return DSOAgent(\n",
        "        intervention_threshold=0.8,    # Base threshold\n",
        "        penalty_multiplier=1.5,        # Base penalty multiplier\n",
        "        price_adjustment_factor=0.2,   # Base price adjustments\n",
        "        grid_stability_weight=0.6,     # Slightly higher weight on stability\n",
        "        market_efficiency_weight=0.4,  # Slightly lower weight on efficiency\n",
        "        intervention_frequency=0.4,    # Higher intervention frequency\n",
        "        adaptive_threshold=True        # Adaptive threshold based on conditions\n",
        "    )\n",
        "\n",
        "# Display DSO strategies\n",
        "print(\"üèõÔ∏è DSO Intervention Strategies:\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "dso_strategies = {\n",
        "    \"Permissive\": create_permissive_dso(),\n",
        "    \"Moderate\": create_moderate_dso(),\n",
        "    \"Strict\": create_strict_dso(),\n",
        "    \"Dynamic\": create_dynamic_dso()\n",
        "}\n",
        "\n",
        "for strategy_name, dso in dso_strategies.items():\n",
        "    print(f\"\\n{strategy_name} DSO:\")\n",
        "    print(f\"  Intervention Threshold: {dso.intervention_threshold:.1%}\")\n",
        "    print(f\"  Penalty Multiplier: {dso.penalty_multiplier:.1f}x\")\n",
        "    print(f\"  Price Adjustment Factor: {dso.price_adjustment_factor:.1%}\")\n",
        "    print(f\"  Grid Stability Weight: {dso.grid_stability_weight:.1%}\")\n",
        "    print(f\"  Market Efficiency Weight: {dso.market_efficiency_weight:.1%}\")\n",
        "    print(f\"  Intervention Frequency: {dso.intervention_frequency:.1%}\")\n",
        "    print(f\"  Adaptive Threshold: {dso.adaptive_threshold}\")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 80)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üë• Agent Creation\n",
        "\n",
        "Now let's create diverse agent configurations that represent realistic market participants. We'll create 8 different agent types to ensure realistic market dynamics and test how different DSO intervention strategies affect various agent profiles.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def create_standard_agents() -> List[DERAgent]:\n",
        "    \"\"\"Create standard agent configuration for DSO intervention analysis.\"\"\"\n",
        "    agents = []\n",
        "    profile_handler = DERProfileHandler()\n",
        "    \n",
        "    print(\"üèóÔ∏è Creating agents for DSO intervention analysis...\")\n",
        "    \n",
        "    # Mix of agent types to create realistic market stress\n",
        "    agent_configs = [\n",
        "        # High-generation agents (potential grid stress)\n",
        "        {\"id\": \"high_gen_001\", \"capacity\": 90.0, \"battery_cap\": 45.0, \"demand_mult\": 0.8},\n",
        "        {\"id\": \"high_gen_002\", \"capacity\": 85.0, \"battery_cap\": 40.0, \"demand_mult\": 0.7},\n",
        "        \n",
        "        # High-demand agents (potential grid stress)\n",
        "        {\"id\": \"high_dem_001\", \"capacity\": 40.0, \"battery_cap\": 20.0, \"demand_mult\": 2.0},\n",
        "        {\"id\": \"high_dem_002\", \"capacity\": 35.0, \"battery_cap\": 15.0, \"demand_mult\": 1.8},\n",
        "        \n",
        "        # Balanced prosumers\n",
        "        {\"id\": \"balanced_001\", \"capacity\": 60.0, \"battery_cap\": 30.0, \"demand_mult\": 1.1},\n",
        "        {\"id\": \"balanced_002\", \"capacity\": 55.0, \"battery_cap\": 25.0, \"demand_mult\": 1.2},\n",
        "        \n",
        "        # Flexible agents with larger batteries (good for grid services)\n",
        "        {\"id\": \"flexible_001\", \"capacity\": 70.0, \"battery_cap\": 60.0, \"demand_mult\": 1.0},\n",
        "        {\"id\": \"flexible_002\", \"capacity\": 65.0, \"battery_cap\": 55.0, \"demand_mult\": 1.1}\n",
        "    ]\n",
        "    \n",
        "    for i, config in enumerate(agent_configs, 1):\n",
        "        print(f\"  Creating agent {i}/8: {config['id']}\")\n",
        "        \n",
        "        generation, demand = profile_handler.get_energy_profiles(\n",
        "            Case3Scenarios.MAX_STEPS,\n",
        "            config[\"capacity\"]\n",
        "        )\n",
        "        \n",
        "        # Apply demand multiplier based on agent type\n",
        "        demand = [d * config[\"demand_mult\"] for d in demand]\n",
        "        \n",
        "        agent = DERAgent(\n",
        "            id=config[\"id\"],\n",
        "            capacity=config[\"capacity\"],\n",
        "            battery=Battery(\n",
        "                nominal_capacity=config[\"battery_cap\"],\n",
        "                min_soc=0.1,\n",
        "                max_soc=0.9,\n",
        "                charge_efficiency=0.94,\n",
        "                discharge_efficiency=0.94\n",
        "            ),\n",
        "            generation_profile=generation,\n",
        "            demand_profile=demand\n",
        "        )\n",
        "        agents.append(agent)\n",
        "    \n",
        "    print(f\"‚úÖ Created {len(agents)} agents successfully!\")\n",
        "    return agents\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create the standard agents\n",
        "agents = create_standard_agents()\n",
        "\n",
        "# Display agent summary\n",
        "print(\"\\nüìä Agent Summary:\")\n",
        "print(\"=\" * 80)\n",
        "for agent in agents:\n",
        "    battery_info = f\"Battery: {agent.battery.nominal_capacity:.1f} kWh\" if agent.battery else \"No Battery\"\n",
        "    print(f\"ID: {agent.id:<15} | Capacity: {agent.capacity:>6.1f} kW | {battery_info}\")\n",
        "    \n",
        "print(\"=\" * 80)\n",
        "total_capacity = sum(agent.capacity for agent in agents)\n",
        "total_battery = sum(agent.battery.nominal_capacity for agent in agents if agent.battery)\n",
        "print(f\"Total Generation Capacity: {total_capacity:.1f} kW\")\n",
        "print(f\"Total Battery Capacity: {total_battery:.1f} kWh\")\n",
        "print(f\"Number of Agents: {len(agents)}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üîÑ Scenario Generation\n",
        "\n",
        "Now let's create scenarios for each DSO intervention strategy to understand how different regulatory approaches affect market dynamics and grid stability.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def create_base_grid_network() -> GridNetwork:\n",
        "    \"\"\"Create base grid network configuration using IEEE34 topology.\"\"\"\n",
        "    return GridNetwork(\n",
        "        topology=GridTopology.IEEE34,\n",
        "        num_nodes=Case3Scenarios.NUM_AGENTS,\n",
        "        capacity=Case3Scenarios.GRID_CAPACITY,\n",
        "        seed=42\n",
        "    )\n",
        "\n",
        "def create_base_market_config() -> MarketConfig:\n",
        "    \"\"\"Create base market configuration for DSO intervention analysis.\"\"\"\n",
        "    return MarketConfig(\n",
        "        min_price=Case3Scenarios.MIN_PRICE,\n",
        "        max_price=Case3Scenarios.MAX_PRICE,\n",
        "        min_quantity=Case3Scenarios.MIN_QUANTITY,\n",
        "        max_quantity=Case3Scenarios.MAX_QUANTITY,\n",
        "        price_mechanism=ClearingMechanism.BID_ASK_SPREAD,  # Market-driven pricing\n",
        "        enable_partner_preference=True,  # Enable strategic partner selection\n",
        "        blockchain_difficulty=2,\n",
        "        visualize_blockchain=False\n",
        "    )\n",
        "\n",
        "def get_all_scenarios() -> Dict[str, Dict[str, Any]]:\n",
        "    \"\"\"Generate all Case 3 scenarios for different DSO intervention strategies.\"\"\"\n",
        "    \n",
        "    scenarios = {}\n",
        "    agents = create_standard_agents()\n",
        "    grid_network = create_base_grid_network()\n",
        "    market_config = create_base_market_config()\n",
        "    der_profile_handler = DERProfileHandler()\n",
        "    \n",
        "    print(\"üîÑ Creating scenarios for all DSO intervention strategies...\")\n",
        "    \n",
        "    # Create scenarios for each DSO strategy\n",
        "    dso_strategy_configs = [\n",
        "        (\"permissive_dso\", create_permissive_dso()),\n",
        "        (\"moderate_dso\", create_moderate_dso()),\n",
        "        (\"strict_dso\", create_strict_dso()),\n",
        "        (\"dynamic_dso\", create_dynamic_dso())\n",
        "    ]\n",
        "    \n",
        "    for i, (strategy_name, dso_agent) in enumerate(dso_strategy_configs, 1):\n",
        "        print(f\"  Creating scenario {i}/4: {strategy_name}\")\n",
        "        \n",
        "        # Create DSO profile handler with specific DSO agent\n",
        "        dso_profile_handler = DSOProfileHandler(\n",
        "            min_price=Case3Scenarios.MIN_PRICE,\n",
        "            max_price=Case3Scenarios.MAX_PRICE,\n",
        "            dso_agent=dso_agent\n",
        "        )\n",
        "        \n",
        "        scenario_config = {\n",
        "            \"max_steps\": Case3Scenarios.MAX_STEPS,\n",
        "            \"agents\": agents.copy(),  # Use copy to avoid shared state\n",
        "            \"market_config\": market_config,\n",
        "            \"grid_network\": grid_network,\n",
        "            \"der_profile_handler\": der_profile_handler,\n",
        "            \"dso_profile_handler\": dso_profile_handler,\n",
        "            \"enable_reset_dso_profiles\": True,\n",
        "            \"enable_asynchronous_order\": True,\n",
        "            \"max_error\": 0.15,  # Moderate error tolerance for DSO intervention\n",
        "            \"num_anchor\": 5,  # Moderate anchors for DSO analysis\n",
        "            \"seed\": 42\n",
        "        }\n",
        "        \n",
        "        scenarios[strategy_name] = scenario_config\n",
        "    \n",
        "    print(f\"‚úÖ Created {len(scenarios)} scenarios successfully!\")\n",
        "    return scenarios\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Generate all scenarios\n",
        "scenarios = get_all_scenarios()\n",
        "\n",
        "# Display scenario summary\n",
        "print(\"\\nüìä Scenario Summary:\")\n",
        "print(\"=\" * 80)\n",
        "for scenario_name, config in scenarios.items():\n",
        "    strategy_name = scenario_name.replace(\"_\", \" \").title()\n",
        "    dso_agent = config['dso_profile_handler'].dso_agent\n",
        "    print(f\"Scenario: {scenario_name}\")\n",
        "    print(f\"  Strategy: {strategy_name}\")\n",
        "    print(f\"  Agents: {len(config['agents'])}\")\n",
        "    print(f\"  Max Steps: {config['max_steps']}\")\n",
        "    print(f\"  Price Range: ${config['market_config'].min_price} - ${config['market_config'].max_price} /MWh\")\n",
        "    print(f\"  DSO Intervention Threshold: {dso_agent.intervention_threshold:.1%}\")\n",
        "    print(f\"  DSO Penalty Multiplier: {dso_agent.penalty_multiplier:.1f}x\")\n",
        "    print()\n",
        "\n",
        "print(f\"Total scenarios created: {len(scenarios)}\")\n",
        "print(\"=\" * 80)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üéØ Training & Evaluation\n",
        "\n",
        "Now let's train each DSO intervention scenario to understand how different regulatory approaches affect agent learning, coordination, and grid stability.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Agent Behavior Options\n",
        "\n",
        "We provide two options for agent behavior:\n",
        "\n",
        "1. **Zero Intelligence Agents (Default)** - Agents use uniform random distribution for bidding decisions, making it easier to visualize DSO intervention effects\n",
        "2. **MARL Training** - Agents learn optimal strategies through reinforcement learning\n",
        "\n",
        "The zero intelligence option serves as a baseline and makes it easier to observe the pure effects of different DSO intervention strategies without the complexity of learning dynamics.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Configuration: Choose agent behavior type\n",
        "USE_ZERO_INTELLIGENCE = True  # Set to False for MARL training\n",
        "\n",
        "print(\"ü§ñ Agent Behavior Configuration:\")\n",
        "print(\"=\" * 50)\n",
        "if USE_ZERO_INTELLIGENCE:\n",
        "    print(\"‚úÖ Using Zero Intelligence Agents (Default)\")\n",
        "    print(\"  ‚Ä¢ Uniform random distribution for bidding\")\n",
        "    print(\"  ‚Ä¢ Easier to visualize DSO intervention effects\")\n",
        "    print(\"  ‚Ä¢ No learning dynamics complexity\")\n",
        "    print(\"  ‚Ä¢ Faster execution for demonstration\")\n",
        "else:\n",
        "    print(\"üß† Using MARL Training\")\n",
        "    print(\"  ‚Ä¢ Agents learn optimal strategies\")\n",
        "    print(\"  ‚Ä¢ Reinforcement learning approach\")\n",
        "    print(\"  ‚Ä¢ More realistic agent behavior\")\n",
        "    print(\"  ‚Ä¢ Longer training time required\")\n",
        "\n",
        "print(f\"\\nCurrent setting: {'Zero Intelligence' if USE_ZERO_INTELLIGENCE else 'MARL Training'}\")\n",
        "print(\"=\" * 50)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Training configuration\n",
        "TRAINING_EPISODES = 200  # Reduced for demonstration\n",
        "EVALUATION_EPISODES = 50\n",
        "ALGORITHM = RLAlgorithm.PPO\n",
        "TRAINING_MODE = TrainingMode.CTDE\n",
        "\n",
        "# Store training results\n",
        "training_results = {}\n",
        "\n",
        "print(f\"üéØ Training Configuration:\")\n",
        "print(f\"  Algorithm: {ALGORITHM.name}\")\n",
        "print(f\"  Training Mode: {TRAINING_MODE.name}\")\n",
        "print(f\"  Training Episodes: {TRAINING_EPISODES}\")\n",
        "print(f\"  Evaluation Episodes: {EVALUATION_EPISODES}\")\n",
        "print(f\"  Scenarios to Train: {len(scenarios)}\")\n",
        "print(f\"  Agent Behavior: {'Zero Intelligence' if USE_ZERO_INTELLIGENCE else 'MARL Training'}\")\n",
        "print()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Modified training section with zero intelligence option\n",
        "if USE_ZERO_INTELLIGENCE:\n",
        "    print(\"üöÄ Running Zero Intelligence Agent Simulations...\")\n",
        "    print(\"=\" * 80)\n",
        "    \n",
        "    # Store training results\n",
        "    training_results = {}\n",
        "    \n",
        "    for i, (scenario_name, config) in enumerate(scenarios.items(), 1):\n",
        "        print(f\"\\nüìà Running Scenario {i}/{len(scenarios)}: {scenario_name}\")\n",
        "        print(\"-\" * 60)\n",
        "        \n",
        "        try:\n",
        "            # For zero intelligence, we'll use proper environment stepping with random actions\n",
        "            print(f\"  üîÑ Running zero intelligence simulation...\")\n",
        "            \n",
        "            # Create trainer to get access to environment\n",
        "            trainer = RLTrainer(\n",
        "                env_config=config,\n",
        "                algorithm=ALGORITHM,  # Algorithm doesn't matter for zero intelligence\n",
        "                training=TRAINING_MODE,\n",
        "                iters=1  # Minimal iterations since we're not training\n",
        "            )\n",
        "            \n",
        "            # Reset environment\n",
        "            trainer.env.reset()\n",
        "            \n",
        "            # Run simulation with random actions\n",
        "            total_reward = 0.0\n",
        "            episode_rewards = []\n",
        "            \n",
        "            for episode in range(10):  # Run 10 episodes for zero intelligence\n",
        "                episode_reward = 0.0\n",
        "                \n",
        "                for step in range(config['max_steps']):\n",
        "                    # Generate random valid actions for all agents\n",
        "                    actions = {}\n",
        "                    for agent_id in trainer.env.agents:\n",
        "                        # Use action_spaces instead of action_space for DTDE mode\n",
        "                        if hasattr(trainer.env, 'action_spaces') and trainer.env.action_spaces is not None:\n",
        "                            action_space = trainer.env.action_spaces[agent_id]\n",
        "                        else:\n",
        "                            action_space = trainer.env.action_space[agent_id]\n",
        "                        actions[agent_id] = action_space.sample()\n",
        "                    \n",
        "                    # Step the environment\n",
        "                    obs, rewards, terminated, truncated, info = trainer.env.step(actions)\n",
        "                    \n",
        "                    # Accumulate rewards\n",
        "                    step_reward = sum(rewards.values()) if isinstance(rewards, dict) else rewards\n",
        "                    episode_reward += step_reward\n",
        "                    \n",
        "                    if terminated or truncated:\n",
        "                        break\n",
        "                \n",
        "                episode_rewards.append(episode_reward)\n",
        "                total_reward += episode_reward\n",
        "                \n",
        "                # Reset for next episode\n",
        "                trainer.env.reset()\n",
        "            \n",
        "            # Calculate average performance\n",
        "            avg_reward = total_reward / len(episode_rewards)\n",
        "            final_reward = episode_rewards[-1] if episode_rewards else 0.0\n",
        "            \n",
        "            # Store results\n",
        "            training_results[scenario_name] = {\n",
        "                \"trainer\": trainer,\n",
        "                \"config\": config,\n",
        "                \"status\": \"completed\",\n",
        "                \"zero_intelligence\": True,\n",
        "                \"final_reward\": final_reward,\n",
        "                \"avg_reward\": avg_reward,\n",
        "                \"episode_rewards\": episode_rewards\n",
        "            }\n",
        "            \n",
        "            print(f\"  ‚úÖ Zero intelligence simulation completed!\")\n",
        "            print(f\"  üìä Final Reward: {final_reward:.3f}\")\n",
        "            print(f\"  üìä Average Reward: {avg_reward:.3f}\")\n",
        "            print(f\"  üìä Episodes Run: {len(episode_rewards)}\")\n",
        "            \n",
        "        except Exception as e:\n",
        "            print(f\"  ‚ùå Simulation failed: {e}\")\n",
        "            training_results[scenario_name] = {\n",
        "                \"trainer\": None,\n",
        "                \"config\": config,\n",
        "                \"status\": \"failed\",\n",
        "                \"error\": str(e)\n",
        "            }\n",
        "\n",
        "else:\n",
        "    print(\"üöÄ Starting MARL training for all scenarios...\")\n",
        "    print(\"=\" * 80)\n",
        "    \n",
        "    for i, (scenario_name, config) in enumerate(scenarios.items(), 1):\n",
        "        print(f\"\\nüìà Training Scenario {i}/{len(scenarios)}: {scenario_name}\")\n",
        "        print(\"-\" * 60)\n",
        "        \n",
        "        try:\n",
        "            # Create trainer\n",
        "            trainer = RLTrainer(\n",
        "                env_config=config,\n",
        "                algorithm=ALGORITHM,\n",
        "                training=TRAINING_MODE,\n",
        "                iters=TRAINING_EPISODES\n",
        "            )\n",
        "            \n",
        "            # Train the scenario\n",
        "            print(f\"  üîÑ Training with {ALGORITHM.name} algorithm...\")\n",
        "            trainer.train()\n",
        "            \n",
        "            # Store results\n",
        "            training_results[scenario_name] = {\n",
        "                \"trainer\": trainer,\n",
        "                \"config\": config,\n",
        "                \"status\": \"completed\",\n",
        "                \"zero_intelligence\": False\n",
        "            }\n",
        "            \n",
        "            print(f\"  ‚úÖ Training completed successfully!\")\n",
        "            \n",
        "        except Exception as e:\n",
        "            print(f\"  ‚ùå Training failed: {e}\")\n",
        "            training_results[scenario_name] = {\n",
        "                \"trainer\": None,\n",
        "                \"config\": config,\n",
        "                \"status\": \"failed\",\n",
        "                \"error\": str(e),\n",
        "                \"zero_intelligence\": False\n",
        "            }\n",
        "\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"üéâ Training/Simulation completed for all scenarios!\")\n",
        "print(f\"Successful: {sum(1 for r in training_results.values() if r['status'] == 'completed')}\")\n",
        "print(f\"Failed: {sum(1 for r in training_results.values() if r['status'] == 'failed')}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Training configuration\n",
        "TRAINING_EPISODES = 200  # Reduced for demonstration\n",
        "EVALUATION_EPISODES = 50\n",
        "ALGORITHM = RLAlgorithm.PPO\n",
        "TRAINING_MODE = TrainingMode.CTDE\n",
        "\n",
        "print(f\"üéØ Training Configuration:\")\n",
        "print(f\"  Algorithm: {ALGORITHM.name}\")\n",
        "print(f\"  Training Mode: {TRAINING_MODE.name}\")\n",
        "print(f\"  Training Episodes: {TRAINING_EPISODES}\")\n",
        "print(f\"  Evaluation Episodes: {EVALUATION_EPISODES}\")\n",
        "print(f\"  Scenarios to Train: {len(scenarios)}\")\n",
        "print()\n",
        "\n",
        "# Store training results\n",
        "training_results = {}\n",
        "\n",
        "print(\"üöÄ Starting training for all DSO intervention strategies...\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "for i, (scenario_name, config) in enumerate(scenarios.items(), 1):\n",
        "    print(f\"\\nüìà Training Scenario {i}/{len(scenarios)}: {scenario_name}\")\n",
        "    print(\"-\" * 60)\n",
        "    \n",
        "    try:\n",
        "        # Create trainer\n",
        "        trainer = RLTrainer(\n",
        "            env_config=config,\n",
        "            algorithm=ALGORITHM,\n",
        "            training=TRAINING_MODE,\n",
        "            iters=TRAINING_EPISODES\n",
        "        )\n",
        "        \n",
        "        # Train the scenario\n",
        "        print(f\"  üîÑ Training with {ALGORITHM.name} algorithm...\")\n",
        "        trainer.train()\n",
        "        \n",
        "        # Store results\n",
        "        training_results[scenario_name] = {\n",
        "            \"trainer\": trainer,\n",
        "            \"config\": config,\n",
        "            \"status\": \"completed\"\n",
        "        }\n",
        "        \n",
        "        print(f\"  ‚úÖ Training completed successfully!\")\n",
        "        \n",
        "    except Exception as e:\n",
        "        print(f\"  ‚ùå Training failed: {e}\")\n",
        "        training_results[scenario_name] = {\n",
        "            \"trainer\": None,\n",
        "            \"config\": config,\n",
        "            \"status\": \"failed\",\n",
        "            \"error\": str(e)\n",
        "        }\n",
        "\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"üéâ Training completed for all scenarios!\")\n",
        "print(f\"Successful: {sum(1 for r in training_results.values() if r['status'] == 'completed')}\")\n",
        "print(f\"Failed: {sum(1 for r in training_results.values() if r['status'] == 'failed')}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üìä Grid Stability Analysis\n",
        "\n",
        "Let's analyze how different DSO intervention strategies affect grid stability, market participation, and agent behavior.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Calculate grid stability and DSO intervention metrics\n",
        "print(\"üìä Grid Stability Analysis\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "grid_stability_data = []\n",
        "\n",
        "for scenario_name, config in scenarios.items():\n",
        "    dso_agent = config['dso_profile_handler'].dso_agent\n",
        "    agents = config['agents']\n",
        "    \n",
        "    # Calculate grid stress indicators\n",
        "    total_capacity = sum(agent.capacity for agent in agents)\n",
        "    total_battery = sum(agent.battery.nominal_capacity for agent in agents if agent.battery)\n",
        "    \n",
        "    # Calculate potential grid stress\n",
        "    max_generation = max(agent.capacity for agent in agents)\n",
        "    max_demand = max(max(agent.demand_profile) for agent in agents)\n",
        "    \n",
        "    # DSO intervention characteristics\n",
        "    intervention_strictness = 1.0 - dso_agent.intervention_threshold  # Higher = stricter\n",
        "    penalty_severity = dso_agent.penalty_multiplier\n",
        "    stability_weight = dso_agent.grid_stability_weight\n",
        "    efficiency_weight = dso_agent.market_efficiency_weight\n",
        "    \n",
        "    grid_stability_data.append({\n",
        "        'Scenario': scenario_name.replace('_', ' ').title(),\n",
        "        'Intervention_Threshold': dso_agent.intervention_threshold,\n",
        "        'Penalty_Multiplier': penalty_severity,\n",
        "        'Stability_Weight': stability_weight,\n",
        "        'Efficiency_Weight': efficiency_weight,\n",
        "        'Intervention_Strictness': intervention_strictness,\n",
        "        'Total_Capacity': total_capacity,\n",
        "        'Total_Battery': total_battery,\n",
        "        'Max_Generation': max_generation,\n",
        "        'Max_Demand': max_demand,\n",
        "        'Grid_Stress_Potential': max_generation / Case3Scenarios.GRID_CAPACITY,\n",
        "        'Adaptive_Threshold': dso_agent.adaptive_threshold\n",
        "    })\n",
        "\n",
        "# Create DataFrame for analysis\n",
        "df_grid_stability = pd.DataFrame(grid_stability_data)\n",
        "\n",
        "print(\"\\nüìä Grid Stability Metrics:\")\n",
        "print(df_grid_stability.to_string(index=False))\n",
        "\n",
        "print(\"\\nüéØ DSO Intervention Analysis:\")\n",
        "for _, row in df_grid_stability.iterrows():\n",
        "    print(f\"\\n{row['Scenario']}:\")\n",
        "    print(f\"  Intervention Threshold: {row['Intervention_Threshold']:.1%}\")\n",
        "    print(f\"  Penalty Multiplier: {row['Penalty_Multiplier']:.1f}x\")\n",
        "    print(f\"  Stability Weight: {row['Stability_Weight']:.1%}\")\n",
        "    print(f\"  Efficiency Weight: {row['Efficiency_Weight']:.1%}\")\n",
        "    print(f\"  Intervention Strictness: {row['Intervention_Strictness']:.1%}\")\n",
        "    print(f\"  Grid Stress Potential: {row['Grid_Stress_Potential']:.1%}\")\n",
        "    print(f\"  Adaptive Threshold: {row['Adaptive_Threshold']}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create grid stability visualization\n",
        "print(\"\\nüìà Creating Grid Stability Visualizations...\")\n",
        "\n",
        "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
        "fig.suptitle('DSO Intervention Strategy Analysis', fontsize=16, fontweight='bold')\n",
        "\n",
        "# Plot 1: Intervention Threshold vs Penalty Multiplier\n",
        "axes[0, 0].scatter(df_grid_stability['Intervention_Threshold'], df_grid_stability['Penalty_Multiplier'], \n",
        "                   s=100, alpha=0.7, c=df_grid_stability['Intervention_Strictness'], cmap='RdYlBu_r')\n",
        "axes[0, 0].set_title('Intervention Threshold vs Penalty Multiplier')\n",
        "axes[0, 0].set_xlabel('Intervention Threshold')\n",
        "axes[0, 0].set_ylabel('Penalty Multiplier')\n",
        "axes[0, 0].grid(True, alpha=0.3)\n",
        "\n",
        "# Plot 2: Stability vs Efficiency Weights\n",
        "axes[0, 1].scatter(df_grid_stability['Stability_Weight'], df_grid_stability['Efficiency_Weight'], \n",
        "                   s=100, alpha=0.7, c=df_grid_stability['Intervention_Strictness'], cmap='RdYlBu_r')\n",
        "axes[0, 1].set_title('Stability Weight vs Efficiency Weight')\n",
        "axes[0, 1].set_xlabel('Grid Stability Weight')\n",
        "axes[0, 1].set_ylabel('Market Efficiency Weight')\n",
        "axes[0, 1].grid(True, alpha=0.3)\n",
        "axes[0, 1].plot([0, 1], [1, 0], 'k--', alpha=0.5, label='Equal Weight Line')\n",
        "axes[0, 1].legend()\n",
        "\n",
        "# Plot 3: Intervention Strictness by Strategy\n",
        "strategy_names = df_grid_stability['Scenario'].tolist()\n",
        "strictness_values = df_grid_stability['Intervention_Strictness'].tolist()\n",
        "colors = ['green', 'orange', 'red', 'blue']\n",
        "axes[1, 0].bar(strategy_names, strictness_values, color=colors, alpha=0.7)\n",
        "axes[1, 0].set_title('Intervention Strictness by Strategy')\n",
        "axes[1, 0].set_ylabel('Intervention Strictness')\n",
        "axes[1, 0].tick_params(axis='x', rotation=45)\n",
        "\n",
        "# Plot 4: Grid Stress Potential\n",
        "stress_values = df_grid_stability['Grid_Stress_Potential'].tolist()\n",
        "axes[1, 1].bar(strategy_names, stress_values, color=colors, alpha=0.7)\n",
        "axes[1, 1].set_title('Grid Stress Potential by Strategy')\n",
        "axes[1, 1].set_ylabel('Grid Stress Potential')\n",
        "axes[1, 1].tick_params(axis='x', rotation=45)\n",
        "axes[1, 1].axhline(y=1.0, color='red', linestyle='--', alpha=0.7, label='Grid Capacity Limit')\n",
        "axes[1, 1].legend()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"\\nüéØ Key Grid Stability Insights:\")\n",
        "print(\"=\" * 50)\n",
        "for _, row in df_grid_stability.iterrows():\n",
        "    print(f\"\\n{row['Scenario']}:\")\n",
        "    if row['Intervention_Strictness'] > 0.3:\n",
        "        print(f\"  ‚ö†Ô∏è  High intervention strictness - may reduce market efficiency\")\n",
        "    elif row['Intervention_Strictness'] > 0.2:\n",
        "        print(f\"  ‚öñÔ∏è  Moderate intervention strictness - balanced approach\")\n",
        "    else:\n",
        "        print(f\"  ‚úÖ Low intervention strictness - high market freedom\")\n",
        "    \n",
        "    print(f\"  üìä Grid stress potential: {row['Grid_Stress_Potential']:.1%}\")\n",
        "    print(f\"  üîÑ Stability vs Efficiency: {row['Stability_Weight']:.1%} vs {row['Efficiency_Weight']:.1%}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üìä Results Analysis\n",
        "\n",
        "Let's analyze the training results to understand how different DSO intervention strategies affect agent behavior, market participation, and grid stability.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Analyze training results\n",
        "print(\"üìä Training Results Analysis\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "successful_scenarios = [name for name, result in training_results.items() if result['status'] == 'completed']\n",
        "failed_scenarios = [name for name, result in training_results.items() if result['status'] == 'failed']\n",
        "\n",
        "print(f\"‚úÖ Successful Scenarios ({len(successful_scenarios)}):\")\n",
        "for scenario in successful_scenarios:\n",
        "    strategy_name = scenario.replace(\"_\", \" \").title()\n",
        "    print(f\"  - {strategy_name}\")\n",
        "\n",
        "if failed_scenarios:\n",
        "    print(f\"\\n‚ùå Failed Scenarios ({len(failed_scenarios)}):\")\n",
        "    for scenario in failed_scenarios:\n",
        "        strategy_name = scenario.replace(\"_\", \" \").title()\n",
        "        error = training_results[scenario]['error']\n",
        "        print(f\"  - {strategy_name}: {error}\")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 80)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create performance comparison plots\n",
        "if successful_scenarios:\n",
        "    print(\"üìà Creating Performance Comparison Plots...\")\n",
        "    \n",
        "    # Extract performance metrics for comparison\n",
        "    performance_data = []\n",
        "    \n",
        "    for scenario_name in successful_scenarios:\n",
        "        trainer = training_results[scenario_name]['trainer']\n",
        "        strategy_name = scenario_name.replace(\"_\", \" \").title()\n",
        "        \n",
        "        # Get DSO intervention metrics for this scenario\n",
        "        dso_row = df_grid_stability[df_grid_stability['Scenario'] == strategy_name].iloc[0]\n",
        "        \n",
        "        # Extract training metrics (if available)\n",
        "        if hasattr(trainer, 'training_history') and trainer.training_history:\n",
        "            final_reward = trainer.training_history[-1] if trainer.training_history else 0\n",
        "            avg_reward = np.mean(trainer.training_history) if trainer.training_history else 0\n",
        "        else:\n",
        "            final_reward = 0\n",
        "            avg_reward = 0\n",
        "        \n",
        "        performance_data.append({\n",
        "            'DSO_Strategy': strategy_name,\n",
        "            'Final_Reward': final_reward,\n",
        "            'Average_Reward': avg_reward,\n",
        "            'Intervention_Strictness': dso_row['Intervention_Strictness'],\n",
        "            'Stability_Weight': dso_row['Stability_Weight'],\n",
        "            'Efficiency_Weight': dso_row['Efficiency_Weight'],\n",
        "            'Penalty_Multiplier': dso_row['Penalty_Multiplier']\n",
        "        })\n",
        "    \n",
        "    # Create DataFrame for analysis\n",
        "    df_performance = pd.DataFrame(performance_data)\n",
        "    \n",
        "    print(\"\\nüìä Performance Summary:\")\n",
        "    print(df_performance.to_string(index=False))\n",
        "    \n",
        "    # Create visualization\n",
        "    fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
        "    fig.suptitle('DSO Intervention Strategy Performance Analysis', fontsize=16, fontweight='bold')\n",
        "    \n",
        "    # Plot 1: Performance vs Intervention Strictness\n",
        "    axes[0, 0].scatter(df_performance['Intervention_Strictness'], df_performance['Final_Reward'], \n",
        "                       s=100, alpha=0.7, c=df_performance['Stability_Weight'], cmap='RdYlBu_r')\n",
        "    axes[0, 0].set_title('Performance vs Intervention Strictness')\n",
        "    axes[0, 0].set_xlabel('Intervention Strictness')\n",
        "    axes[0, 0].set_ylabel('Final Reward')\n",
        "    axes[0, 0].grid(True, alpha=0.3)\n",
        "    \n",
        "    # Plot 2: Performance by DSO Strategy\n",
        "    strategy_names = df_performance['DSO_Strategy'].tolist()\n",
        "    final_rewards = df_performance['Final_Reward'].tolist()\n",
        "    colors = ['green', 'orange', 'red', 'blue']\n",
        "    axes[0, 1].bar(strategy_names, final_rewards, color=colors, alpha=0.7)\n",
        "    axes[0, 1].set_title('Final Reward by DSO Strategy')\n",
        "    axes[0, 1].set_ylabel('Final Reward')\n",
        "    axes[0, 1].tick_params(axis='x', rotation=45)\n",
        "    \n",
        "    # Plot 3: Stability vs Efficiency Trade-off\n",
        "    axes[1, 0].scatter(df_performance['Stability_Weight'], df_performance['Efficiency_Weight'], \n",
        "                       s=100, alpha=0.7, c=df_performance['Final_Reward'], cmap='viridis')\n",
        "    axes[1, 0].set_title('Stability vs Efficiency Trade-off')\n",
        "    axes[1, 0].set_xlabel('Grid Stability Weight')\n",
        "    axes[1, 0].set_ylabel('Market Efficiency Weight')\n",
        "    axes[1, 0].grid(True, alpha=0.3)\n",
        "    axes[1, 0].plot([0, 1], [1, 0], 'k--', alpha=0.5, label='Equal Weight Line')\n",
        "    axes[1, 0].legend()\n",
        "    \n",
        "    # Plot 4: Performance Ranking\n",
        "    sorted_df = df_performance.sort_values('Final_Reward', ascending=True)\n",
        "    axes[1, 1].barh(sorted_df['DSO_Strategy'], sorted_df['Final_Reward'], \n",
        "                    color=[colors[i] for i in range(len(sorted_df))], alpha=0.7)\n",
        "    axes[1, 1].set_title('DSO Strategy Performance Ranking')\n",
        "    axes[1, 1].set_xlabel('Final Reward')\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "    \n",
        "    print(\"\\nüéØ Key Performance Insights:\")\n",
        "    best_strategy = df_performance.loc[df_performance['Final_Reward'].idxmax()]\n",
        "    worst_strategy = df_performance.loc[df_performance['Final_Reward'].idxmin()]\n",
        "    \n",
        "    print(f\"  üèÜ Best Performing Strategy: {best_strategy['DSO_Strategy']} (Reward: {best_strategy['Final_Reward']:.2f})\")\n",
        "    print(f\"  üìâ Lowest Performing Strategy: {worst_strategy['DSO_Strategy']} (Reward: {worst_strategy['Final_Reward']:.2f})\")\n",
        "    print(f\"  üìä Performance Range: {df_performance['Final_Reward'].max() - df_performance['Final_Reward'].min():.2f}\")\n",
        "    \n",
        "    # DSO intervention impact analysis\n",
        "    print(f\"\\nüìà DSO Intervention Impact:\")\n",
        "    for _, row in df_performance.iterrows():\n",
        "        print(f\"  {row['DSO_Strategy']}: Strictness={row['Intervention_Strictness']:.1%}, Reward={row['Final_Reward']:.2f}\")\n",
        "    \n",
        "else:\n",
        "    print(\"‚ùå No successful training results to analyze.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üî¨ Research Implications\n",
        "\n",
        "Based on our analysis of different DSO intervention strategies, let's discuss the research implications and expected outcomes.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### DSO Intervention Analysis\n",
        "\n",
        "**Key Findings:**\n",
        "- Different DSO intervention strategies lead to distinct agent behavioral patterns\n",
        "- Intervention strictness affects market participation and grid stability\n",
        "- Balance between market freedom and grid stability is crucial for optimal performance\n",
        "\n",
        "**DSO Strategy-Specific Insights:**\n",
        "\n",
        "1. **Permissive DSO:**\n",
        "   - High market freedom with minimal intervention\n",
        "   - May lead to higher market efficiency but potential grid instability\n",
        "   - Agents have more flexibility in their strategies\n",
        "   - Risk of grid constraints during peak periods\n",
        "\n",
        "2. **Moderate DSO:**\n",
        "   - Balanced approach between market freedom and grid stability\n",
        "   - Standard intervention thresholds and penalty structures\n",
        "   - Good compromise for most market conditions\n",
        "   - Predictable regulatory environment\n",
        "\n",
        "3. **Strict DSO:**\n",
        "   - High intervention with strict grid constraints\n",
        "   - Improved grid stability but may reduce market efficiency\n",
        "   - Agents may adopt more conservative strategies\n",
        "   - Higher penalty costs for constraint violations\n",
        "\n",
        "4. **Dynamic DSO:**\n",
        "   - Adaptive intervention based on real-time conditions\n",
        "   - Potential for optimal balance between stability and efficiency\n",
        "   - More complex regulatory environment\n",
        "   - Requires sophisticated monitoring and control systems\n",
        "\n",
        "### Grid Stability Insights\n",
        "\n",
        "**Intervention Effectiveness:**\n",
        "- Strict intervention improves grid stability but reduces market efficiency\n",
        "- Permissive intervention allows higher efficiency but risks grid instability\n",
        "- Dynamic intervention may provide optimal balance\n",
        "- Intervention timing and thresholds are critical factors\n",
        "\n",
        "**Agent Adaptation:**\n",
        "- Agents adapt their strategies to DSO intervention levels\n",
        "- Higher penalties lead to more conservative agent behavior\n",
        "- Market participation may decrease with strict intervention\n",
        "- Agent learning convergence varies with intervention strictness\n",
        "\n",
        "### Market Efficiency Analysis\n",
        "\n",
        "**Trade-offs:**\n",
        "- Market efficiency vs. grid stability trade-off\n",
        "- Intervention strictness affects agent participation\n",
        "- Penalty structures influence agent behavior\n",
        "- Dynamic intervention may optimize both objectives\n",
        "\n",
        "**Performance Metrics:**\n",
        "- Social welfare varies with intervention strategy\n",
        "- Market liquidity affected by intervention frequency\n",
        "- Price volatility influenced by DSO policies\n",
        "- Agent coordination effectiveness impacted by regulatory environment\n",
        "\n",
        "### Policy Implications\n",
        "\n",
        "**Regulatory Framework Design:**\n",
        "- Results inform optimal DSO intervention strategies\n",
        "- Provide insights on intervention threshold design\n",
        "- Guide development of penalty structures\n",
        "- Support dynamic intervention implementation\n",
        "\n",
        "**Market Design Decisions:**\n",
        "- Balance between market freedom and grid stability\n",
        "- Intervention frequency and timing optimization\n",
        "- Penalty structure design for effective deterrence\n",
        "- Adaptive intervention system implementation\n",
        "\n",
        "**Implementation Considerations:**\n",
        "- Monitoring and control system requirements\n",
        "- Communication protocols for dynamic intervention\n",
        "- Agent education and adaptation support\n",
        "- Regulatory framework flexibility and adaptability\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üìù Summary & Next Steps\n",
        "\n",
        "### Case Study 3 Summary\n",
        "\n",
        "This notebook demonstrated a comprehensive analysis of DSO intervention strategies in decentralized local energy markets. We:\n",
        "\n",
        "1. **Created four distinct DSO intervention strategies** representing different regulatory approaches\n",
        "2. **Implemented grid stability analysis** using intervention metrics and thresholds\n",
        "3. **Trained agents using MARL** to understand behavioral differences\n",
        "4. **Analyzed performance metrics** to identify optimal intervention strategies\n",
        "5. **Discussed research implications** for regulatory framework design\n",
        "\n",
        "### Key Contributions\n",
        "\n",
        "- **Systematic DSO intervention comparison** with controlled variables\n",
        "- **Quantitative grid stability analysis** using standard regulatory metrics\n",
        "- **Agent behavior insights** for regulatory framework optimization\n",
        "- **Policy recommendations** for DSO intervention design\n",
        "\n",
        "### Next Steps\n",
        "\n",
        "1. **Run additional training paradigms** (CTCE, DTDE) for comprehensive validation\n",
        "2. **Extend analysis** to include more detailed grid stability metrics\n",
        "3. **Test robustness** under different grid conditions and agent configurations\n",
        "4. **Compare with other case studies** to understand intervention interactions\n",
        "\n",
        "### Related Case Studies\n",
        "\n",
        "- **[Case 1: Market Mechanism Comparison](case1_market_mechanisms.ipynb)** - How mechanisms interact with DSO intervention\n",
        "- **[Case 2: Agent Heterogeneity](case2_agent_heterogeneity.ipynb)** - Market power effects on intervention effectiveness\n",
        "- **[Case 4: Grid Topology](case4_grid_constraints.ipynb)** - Physical constraints and intervention strategies\n",
        "- **[Case 6: Implicit Cooperation](case6_implicit_cooperation.ipynb)** - Core research validation\n",
        "\n",
        "---\n",
        "\n",
        "**üéØ Ready to explore the next case study? Navigate to the [Case Studies Index](case_studies_index.ipynb) to continue your research journey!**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
